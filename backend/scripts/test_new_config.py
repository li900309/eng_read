#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çš„é…ç½®æ–‡ä»¶åŠ è½½åŠŸèƒ½

éªŒè¯LLMæœåŠ¡èƒ½å¦æ­£ç¡®åŠ è½½æ–°çš„JSONé…ç½®æ ¼å¼ã€‚
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
projectRoot = Path(__file__).parent.parent
sys.path.insert(0, str(projectRoot))

from app.services import getLLMServiceWithConfig, getLLMService, resetLLMServices


def testNewConfigFormat():
    """æµ‹è¯•æ–°é…ç½®æ ¼å¼"""
    print("=== æµ‹è¯•æ–°çš„JSONé…ç½®æ ¼å¼ ===\n")

    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    configFile = projectRoot / 'test_api_providers.json'

    if not configFile.exists():
        print(f"âœ— æµ‹è¯•é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {configFile}")
        return False

    try:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶: {configFile}")
        llmService = getLLMServiceWithConfig(str(configFile))

        # æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
        providers = llmService.configManager.getAllProviders()
        print(f"âœ“ æˆåŠŸåŠ è½½ {len(providers)} ä¸ªä¾›åº”å•†")

        for providerName, providerConfig in providers.items():
            print(f"\nä¾›åº”å•†: {providerName}")
            print(f"  åç§°: {providerConfig.get('name', 'Unknown')}")
            print(f"  åŸºç¡€URL: {providerConfig.get('base_url', 'Unknown')}")

            models = providerConfig.get('models', {})
            for modelName, modelConfig in models.items():
                print(f"  æ¨¡å‹: {modelName}")
                print(f"    æœ€å¤§tokens: {modelConfig.get('max_tokens', 'Unknown')}")
                print(f"    æ¸©åº¦: {modelConfig.get('temperature', 'Unknown')}")

        return True

    except Exception as e:
        print(f"âœ— åŠ è½½é…ç½®å¤±è´¥: {e}")
        return False


def testDirectConfigData():
    """æµ‹è¯•ç›´æ¥ä¼ å…¥é…ç½®æ•°æ®"""
    print("\n=== æµ‹è¯•ç›´æ¥ä¼ å…¥é…ç½®æ•°æ® ===\n")

    # æ¨¡æ‹Ÿé…ç½®æ•°æ®
    configData = [
        {
            "test-model": {
                "baseurl": "https://api.test.com/v1/",
                "modelname": "Test Model",
                "apikey": "test-key-123",
                "max_tokens": 2000,
                "temperature": 0.5
            }
        }
    ]

    try:
        print("åˆ›å»ºLLMæœåŠ¡å®ä¾‹...")
        llmService = getLLMService(configData=configData)

        # æ£€æŸ¥é…ç½®
        firstProvider = llmService.configManager.getFirstAvailableProvider()
        if firstProvider:
            print(f"âœ“ ç¬¬ä¸€ä¸ªå¯ç”¨ä¾›åº”å•†: {firstProvider}")

            # è·å–æ¨¡å‹é…ç½®
            models = llmService.configManager.getProvider(firstProvider).get('models', {})
            if models:
                firstModel = list(models.keys())[0]
                print(f"âœ“ ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹: {firstModel}")

                return True

        print("âœ— æœªæ‰¾åˆ°å¯ç”¨çš„ä¾›åº”å•†æˆ–æ¨¡å‹")
        return False

    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        return False


def testTemplateWithNewConfig():
    """æµ‹è¯•ä½¿ç”¨æ–°é…ç½®è¿›è¡Œæ¨¡æ¿èŠå¤©"""
    print("\n=== æµ‹è¯•æ¨¡æ¿èŠå¤©åŠŸèƒ½ ===\n")

    configFile = projectRoot / 'test_api_providers.json'

    if not configFile.exists():
        print(f"âœ— æµ‹è¯•é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {configFile}")
        return False

    try:
        # é‡ç½®æœåŠ¡å®ä¾‹
        resetLLMServices()

        # ä½¿ç”¨æ–°é…ç½®åˆ›å»ºæœåŠ¡
        llmService = getLLMServiceWithConfig(str(configFile))

        from app.services import PromptTemplates

        # ä½¿ç”¨é˜…è¯»åŠ©æ‰‹æ¨¡æ¿
        print("ä½¿ç”¨é˜…è¯»åŠ©æ‰‹æ¨¡æ¿å‘é€è¯·æ±‚...")
        response = llmService.chatWithTemplate(
            PromptTemplates.readingAssistant(
                "The quick brown fox jumps over the lazy dog.",
                3
            )
        )

        if response.success:
            print("âœ“ æ¨¡æ¿èŠå¤©æˆåŠŸ")
            print(f"  å“åº”å†…å®¹: {response.content[:100]}...")
            return True
        else:
            print(f"âœ— æ¨¡æ¿èŠå¤©å¤±è´¥: {response.error}")
            return False

    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        return False


def testConfigComparison():
    """æµ‹è¯•é…ç½®æ ¼å¼å¯¹æ¯”"""
    print("\n=== é…ç½®æ ¼å¼å¯¹æ¯”æµ‹è¯• ===\n")

    # ä¼ ç»Ÿé…ç½®æ ¼å¼
    traditionalConfig = {
        "providers": {
            "openai": {
                "name": "OpenAI",
                "base_url": "https://api.openai.com/v1",
                "api_key": "test-key",
                "models": {
                    "gpt-4": {
                        "max_tokens": 4000,
                        "temperature": 0.7
                    }
                }
            }
        }
    }

    # æ–°æ ¼å¼
    newConfig = [
        {
            "glm-4.5-flash": {
                "baseurl": "https://open.bigmodel.cn/api/paas/v4/",
                "modelname": "GLM-4.5-Flash",
                "apikey": "test-key"
            }
        }
    ]

    try:
        # æµ‹è¯•ä¼ ç»Ÿæ ¼å¼
        resetLLMServices()
        service1 = getLLMService(configData=traditionalConfig)
        providers1 = service1.configManager.getAllProviders()
        print(f"âœ“ ä¼ ç»Ÿæ ¼å¼åŠ è½½æˆåŠŸ: {len(providers1)} ä¸ªä¾›åº”å•†")

        # æµ‹è¯•æ–°æ ¼å¼
        service2 = getLLMService(configData=newConfig)
        providers2 = service2.configManager.getAllProviders()
        print(f"âœ“ æ–°æ ¼å¼åŠ è½½æˆåŠŸ: {len(providers2)} ä¸ªä¾›åº”å•†")

        return True

    except Exception as e:
        print(f"âœ— æ ¼å¼å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=== LLMæ–°é…ç½®æ ¼å¼åŠŸèƒ½æµ‹è¯• ===\n")

    tests = [
        ("æ–°é…ç½®æ ¼å¼æµ‹è¯•", testNewConfigFormat),
        ("ç›´æ¥é…ç½®æ•°æ®æµ‹è¯•", testDirectConfigData),
        ("æ¨¡æ¿èŠå¤©æµ‹è¯•", testTemplateWithNewConfig),
        ("é…ç½®æ ¼å¼å¯¹æ¯”æµ‹è¯•", testConfigComparison),
    ]

    results = []
    for testName, testFunc in tests:
        try:
            result = testFunc()
            results.append((testName, result))
        except Exception as e:
            print(f"âœ— {testName}å‡ºç°å¼‚å¸¸: {e}")
            results.append((testName, False))

    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    successCount = 0
    for testName, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{testName}: {status}")
        if result:
            successCount += 1

    print(f"\næ€»è®¡: {successCount}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")

    if successCount == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°é…ç½®æ ¼å¼å·¥ä½œæ­£å¸¸ã€‚")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
        return 1


if __name__ == '__main__':
    sys.exit(main())